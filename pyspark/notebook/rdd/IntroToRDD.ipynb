{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to RDD and ways to create RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is RDD ?\n",
    "\n",
    "Resilient Distributed Dataset is the fundamental data structure of pyspark\n",
    "* Resilient - Immutable and Fault tolerant\n",
    "* Distributed Dataset - Collection of logical partitions that may be processed in different nodes\n",
    "\n",
    "Initialy data will be present in the driver, when we create RDD using the parallelise() method data will be divided into logical partitions and distributed accross the cluster ( executor ). Driver will take note where each partitions is present\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/31 15:49:24 WARN Utils: Your hostname, padmanabhan-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "24/10/31 15:49:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/31 15:49:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>intro-to-rdd</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ef4e0657500>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating spark session \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"intro-to-rdd\").getOrCreate()\n",
    "spark.getActiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating spark context - from pyspark 2.0 we need to create Spark Session first then creact Spark Context\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289\n"
     ]
    }
   ],
   "source": [
    "data = [1,2,3,4,5,6,7,8,9,10]\n",
    "# create logical partitions and distribute the partitions\n",
    "rdd = sc.parallelize(data)       \n",
    "print(rdd)\n",
    "\n",
    "# note: Spark wont perform Transformation until it encounter action like collect(),saveAsTextFile(),etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action : Get first element from RDD =  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Getting details about the RDD\n",
    "\n",
    "print(rdd.getNumPartitions())\n",
    "print(\"Action : Get first element from RDD = \",rdd.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another example of creating rdd using the list of tuples\n",
    "data2 = [('padhu',2000),('karthik',5000),('selva',1000)]\n",
    "rdd2 = sc.parallelize(data2)\n",
    "print(rdd2.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('padhu', 2000)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.take(1) \n",
    "\n",
    "# o/p : [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [('padhu', 2000)], [('karthik', 5000)], [('selva', 1000)]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd2.collect()\n",
    "\n",
    "# default - 4 partitions for rdd2 \n",
    "# To get data from particular rdd partition use glom()\n",
    "\n",
    "partitions = rdd2.glom().collect()\n",
    "\n",
    "print(partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('padhu', 2000)], [('karthik', 5000), ('selva', 1000)]]\n",
      "2\n",
      "check whether rdd is empty :  False\n"
     ]
    }
   ],
   "source": [
    "# overriding default partition count\n",
    "\n",
    "rdd3 = sc.parallelize(data2,numSlices = 2)\n",
    "partition3 = rdd3.glom().collect()\n",
    "print(partition3)\n",
    "print(rdd3.getNumPartitions())\n",
    "print(\"check whether rdd is empty : \",rdd3.isEmpty())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take() - input: number of element to return starting from the RDD\n",
    "rdd3.take(0)\n",
    "\n",
    "# o/p : 0 element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting RDD into DF\n",
    "\n",
    "Methods:\n",
    "* toDF(optional=schema)\n",
    "* createDataFrame(rdd,schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|     _1|  _2|\n",
      "+-------+----+\n",
      "|  padhu|2000|\n",
      "|karthik|5000|\n",
      "|  selva|1000|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = rdd3.toDF()\n",
    "df.show()\n",
    "\n",
    "# note: column name by default _1, _2, _3 ...etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- amount: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# defining column names while converting rdd into df'\n",
    "\n",
    "column_name = [\"name\",\"amount\"]\n",
    "df = rdd3.toDF(column_name)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|name   |amount|\n",
      "+-------+------+\n",
      "|padhu  |2000  |\n",
      "|karthik|5000  |\n",
      "|selva  |1000  |\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert RDD into DF using CreateDataFrame()\n",
    "df = spark.createDataFrame(rdd3,schema=column_name)\n",
    "df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why to use StructType and StructField ?\n",
    "\n",
    "By default while converting the RDD into DF, datatype are interperted from the data and Nullable are True. To override these we can use StructType and StructField ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = false)\n",
      " |-- amount: integer (nullable = true)\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   name|amount|\n",
      "+-------+------+\n",
      "|  padhu|  2000|\n",
      "|karthik|  5000|\n",
      "|  selva|  1000|\n",
      "+-------+------+\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"name\",StringType(),False),\n",
    "    StructField(\"amount\",IntegerType(),True)])\n",
    "\n",
    "df4 = spark.createDataFrame(rdd3,schema= schema)\n",
    "print(df4.printSchema())\n",
    "print(df4.show(truncate= True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BigdataEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
