{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column class:\n",
    "1. It represent single column in the dataframe\n",
    "2. Provide functions to manipulate columns and rows\n",
    "3. Can be used with filter() transformations to filter the dataframe rows\n",
    "4. Can be used with pyspark.sql.functions which take column object and return column type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/12 06:25:58 WARN Utils: Your hostname, padmanabhan-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "24/11/12 06:25:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/12 06:26:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# creating spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"learning\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<'India'>\n"
     ]
    }
   ],
   "source": [
    "# creating col class object\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "colObj = lit(\"India\")\n",
    "print(colObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple ways to access df columns\n",
    "\n",
    "# df.column\n",
    "# df[\"column\"]\n",
    "# df[\"`column`\"]\n",
    "# df.select(col(\"column\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+------+\n",
      "|            name| location|salary|\n",
      "+----------------+---------+------+\n",
      "|{Padmanabhan, s}|  chennai|  1000|\n",
      "|    {karthik, k}|thanjavur|  5000|\n",
      "+----------------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating complex data using Row class, we can also use StructField and StructType \n",
    "from pyspark.sql import Row\n",
    "\n",
    "data = [Row(name=Row(firstName=\"Padmanabhan\",lastName=\"s\"),location=\"chennai\",salary=1000),\n",
    "        Row(name=Row(firstName=\"karthik\",lastName=\"k\"),location=\"thanjavur\",salary=5000)]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|name.firstName|\n",
      "+--------------+\n",
      "|   Padmanabhan|\n",
      "|       karthik|\n",
      "+--------------+\n",
      "\n",
      "+--------+\n",
      "|lastName|\n",
      "+--------+\n",
      "|       s|\n",
      "|       k|\n",
      "+--------+\n",
      "\n",
      "+----------------+---------+------+\n",
      "|            name| location|salary|\n",
      "+----------------+---------+------+\n",
      "|{Padmanabhan, s}|  chennai|  1000|\n",
      "|    {karthik, k}|thanjavur|  5000|\n",
      "+----------------+---------+------+\n",
      "\n",
      "+----------------+---------+------+\n",
      "|            name| location|salary|\n",
      "+----------------+---------+------+\n",
      "|{Padmanabhan, s}|  chennai|  1000|\n",
      "|    {karthik, k}|thanjavur|  5000|\n",
      "+----------------+---------+------+\n",
      "\n",
      "+-----------+--------+\n",
      "|  firstName|lastName|\n",
      "+-----------+--------+\n",
      "|Padmanabhan|       s|\n",
      "|    karthik|       k|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ways to select the nested props\n",
    "from pyspark.sql.functions import col\n",
    "df.select(df.name.firstName).show()\n",
    "df.select(col(\"name.lastName\")).show()\n",
    "df.select(df.columns).show()\n",
    "df.select(\"*\").show()\n",
    "df.select(\"name.*\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|  firstName|\n",
      "+-----------+\n",
      "|Padmanabhan|\n",
      "|    karthik|\n",
      "+-----------+\n",
      "\n",
      "+----------------+---------+------+------------+\n",
      "|            name| location|salary|modifiedName|\n",
      "+----------------+---------+------+------------+\n",
      "|{Padmanabhan, s}|  chennai|  1000|        NULL|\n",
      "|    {karthik, k}|thanjavur|  5000|        NULL|\n",
      "+----------------+---------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.select(df[\"name.firstName\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+------+-------------+\n",
      "|            name| location|salary| modifiedName|\n",
      "+----------------+---------+------+-------------+\n",
      "|{Padmanabhan, s}|  chennai|  1000|Padmanabhan,s|\n",
      "|    {karthik, k}|thanjavur|  5000|    karthik,k|\n",
      "+----------------+---------+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating new column\n",
    "\n",
    "from pyspark.sql.functions import lit,col,concat_ws\n",
    "df.withColumn(\"modifiedName\",concat_ws(\",\",col(\"name.firstName\"),col(\"name.lastName\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------+\n",
      "|(name.firstName AS firstName + name.lastName AS lastName)|\n",
      "+---------------------------------------------------------+\n",
      "|                                                     NULL|\n",
      "|                                                     NULL|\n",
      "+---------------------------------------------------------+\n",
      "\n",
      "+----------------+---------+------+---------------+\n",
      "|            name| location|salary|modified-salary|\n",
      "+----------------+---------+------+---------------+\n",
      "|{Padmanabhan, s}|  chennai|  1000|          10000|\n",
      "|    {karthik, k}|thanjavur|  5000|          10000|\n",
      "+----------------+---------+------+---------------+\n",
      "\n",
      "+----------------+---------+------+---------------+\n",
      "|            name| location|salary|expected-salary|\n",
      "+----------------+---------+------+---------------+\n",
      "|{Padmanabhan, s}|  chennai|  1000|          10000|\n",
      "|    {karthik, k}|thanjavur|  5000|          10000|\n",
      "+----------------+---------+------+---------------+\n",
      "\n",
      "+-----------+\n",
      "|salary-diff|\n",
      "+-----------+\n",
      "|       9000|\n",
      "|       5000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# arithmetics operations on the df columns\n",
    "\n",
    "## pyspark wont support + of string unlike the native string. Use concat or concat_ws\n",
    "df.select(df[\"name.firstName\"]+df[\"name.lastName\"]).show()  # o/p null\n",
    "\n",
    "## Question : create a new column for expected salary and add default 10k\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df2 = df.withColumn(\"modified-salary\",lit(10000))\n",
    "df2.show()\n",
    "\n",
    "## Now modify the column name to expected salary\n",
    "\n",
    "df2 = df2.withColumnRenamed(\"modified-salary\",\"expected-salary\")\n",
    "df2.show()\n",
    "\n",
    "# arithmetic operations on the salary and expected salary columns\n",
    "\n",
    "df2.select((df2[\"expected-salary\"] - df2[\"salary\"]).alias(\"salary-diff\")).show()\n",
    "# supported airthmetic operators: +, - %, /, *, <,>, =="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### commonly used column functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|current-salary|\n",
      "+--------------+\n",
      "|          1000|\n",
      "|          5000|\n",
      "+--------------+\n",
      "\n",
      "+--------------+\n",
      "|current-salary|\n",
      "+--------------+\n",
      "|          1000|\n",
      "|          5000|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# alias, name same like alias\n",
    "\n",
    "df.select(df.salary.alias(\"current-salary\")).show()\n",
    "\n",
    "df.select(df.salary.name(\"current-salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+------+---------------+\n",
      "|            name| location|salary|expected-salary|\n",
      "+----------------+---------+------+---------------+\n",
      "|{Padmanabhan, s}|  chennai|  1000|          10000|\n",
      "|    {karthik, k}|thanjavur|  5000|          10000|\n",
      "+----------------+---------+------+---------------+\n",
      "\n",
      "+----------------+---------+------+---------------+\n",
      "|            name| location|salary|expected-salary|\n",
      "+----------------+---------+------+---------------+\n",
      "|    {karthik, k}|thanjavur|  5000|          10000|\n",
      "|{Padmanabhan, s}|  chennai|  1000|          10000|\n",
      "+----------------+---------+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# asc, desc\n",
    "df2.sort(df[\"salary\"].asc()).show()\n",
    "df2.sort(df.salary.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asc , desc handling null\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BigdataEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
