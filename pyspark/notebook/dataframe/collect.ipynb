{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect()\n",
    "1. It is action operation \n",
    "2. Used to get data from the RDD and Dataframe from all nodes into the driver \n",
    "3. Usually will use after the filter(), group()\n",
    "4. Using collect in larger dataset will result in the out of memory error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/12 23:12:32 WARN Utils: Your hostname, padmanabhan-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "24/11/12 23:12:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/12 23:12:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# creating spark session and df\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"learning\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "# Column names\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data = data, schema = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# collect - retrieve all elements in a dataframe from all nodes and return a row type obj\n",
    "data =df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "Row(firstname='James', lastname='Smith', country='USA', state='CA')\n",
      "Row(firstname='Michael', lastname='Rose', country='USA', state='NY')\n",
      "Row(firstname='Robert', lastname='Williams', country='USA', state='CA')\n",
      "Row(firstname='Maria', lastname='Jones', country='USA', state='FL')\n"
     ]
    }
   ],
   "source": [
    "print(type(data))\n",
    "\n",
    "for i in data:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(firstname='James'),\n",
       " Row(firstname='Michael'),\n",
       " Row(firstname='Robert'),\n",
       " Row(firstname='Maria')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for specific columns\n",
    "\n",
    "df.select(\"firstname\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "data = [(\"padhu@gmail.com\",)]\n",
    "df = spark.createDataFrame(data,schema=[\"email\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = df.toLocalIterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@gmail.com\n"
     ]
    }
   ],
   "source": [
    "iterator = df.toLocalIterator()\n",
    "for i in iterator:\n",
    "    valInd = i.email.index(\"@\")\n",
    "    print(i.email[valInd:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import instr\n",
    "Ind = df.select(instr(df.email,\"@\").alias(\"startInd\")).collect()\n",
    "Ind = Ind[0][\"startInd\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+------+\n",
      "|          email|startInd|endInd|\n",
      "+---------------+--------+------+\n",
      "|padhu@gmail.com|       6|    15|\n",
      "+---------------+--------+------+\n",
      "\n",
      "+---------------+----------+\n",
      "|          email|    answer|\n",
      "+---------------+----------+\n",
      "|padhu@gmail.com|@gmail.com|\n",
      "+---------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+------+----------+\n",
      "|          email|startInd|endInd|   answer2|\n",
      "+---------------+--------+------+----------+\n",
      "|padhu@gmail.com|       6|    15|@gmail.com|\n",
      "+---------------+--------+------+----------+\n",
      "\n"
     ]
    },
    {
     "ename": "PySparkTypeError",
     "evalue": "[NOT_ITERABLE] Column is not iterable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m df4 \u001b[38;5;241m=\u001b[39m df2\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer2\u001b[39m\u001b[38;5;124m\"\u001b[39m,expr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubstring(email,startInd,endInd)\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      9\u001b[0m df4\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 11\u001b[0m df5 \u001b[38;5;241m=\u001b[39m df2\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer3\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[43msubstring\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memail\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstartInd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minteger\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mendInd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minteger\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     12\u001b[0m df5\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/Desktop/development/data-engineering/BigdataEnv/lib/python3.12/site-packages/pyspark/sql/utils.py:174\u001b[0m, in \u001b[0;36mtry_remote_functions.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(functions, f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/development/data-engineering/BigdataEnv/lib/python3.12/site-packages/pyspark/sql/functions.py:9092\u001b[0m, in \u001b[0;36msubstring\u001b[0;34m(str, pos, len)\u001b[0m\n\u001b[1;32m   9056\u001b[0m \u001b[38;5;129m@try_remote_functions\u001b[39m\n\u001b[1;32m   9057\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msubstring\u001b[39m(\u001b[38;5;28mstr\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m, pos: \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mlen\u001b[39m: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Column:\n\u001b[1;32m   9058\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   9059\u001b[0m \u001b[38;5;124;03m    Substring starts at `pos` and is of length `len` when str is String type or\u001b[39;00m\n\u001b[1;32m   9060\u001b[0m \u001b[38;5;124;03m    returns the slice of byte array that starts at `pos` in byte and is of length `len`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9090\u001b[0m \u001b[38;5;124;03m    [Row(s='ab')]\u001b[39;00m\n\u001b[1;32m   9091\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 9092\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_invoke_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubstring\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_to_java_column\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/development/data-engineering/BigdataEnv/lib/python3.12/site-packages/pyspark/sql/functions.py:97\u001b[0m, in \u001b[0;36m_invoke_function\u001b[0;34m(name, *args)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     96\u001b[0m jf \u001b[38;5;241m=\u001b[39m _get_jvm_function(name, SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context)\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(\u001b[43mjf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Desktop/development/data-engineering/BigdataEnv/lib/python3.12/site-packages/py4j/java_gateway.py:1314\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m-> 1314\u001b[0m     args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1316\u001b[0m     command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m         args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m         proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/Desktop/development/data-engineering/BigdataEnv/lib/python3.12/site-packages/py4j/java_gateway.py:1277\u001b[0m, in \u001b[0;36mJavaMember._build_args\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build_args\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m   1276\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconverters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconverters) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1277\u001b[0m         (new_args, temp_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1279\u001b[0m         new_args \u001b[38;5;241m=\u001b[39m args\n",
      "File \u001b[0;32m~/Desktop/development/data-engineering/BigdataEnv/lib/python3.12/site-packages/py4j/java_gateway.py:1264\u001b[0m, in \u001b[0;36mJavaMember._get_args\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m converter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39mconverters:\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m converter\u001b[38;5;241m.\u001b[39mcan_convert(arg):\n\u001b[0;32m-> 1264\u001b[0m         temp_arg \u001b[38;5;241m=\u001b[39m \u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1265\u001b[0m         temp_args\u001b[38;5;241m.\u001b[39mappend(temp_arg)\n\u001b[1;32m   1266\u001b[0m         new_args\u001b[38;5;241m.\u001b[39mappend(temp_arg)\n",
      "File \u001b[0;32m~/Desktop/development/data-engineering/BigdataEnv/lib/python3.12/site-packages/py4j/java_collections.py:510\u001b[0m, in \u001b[0;36mListConverter.convert\u001b[0;34m(self, object, gateway_client)\u001b[0m\n\u001b[1;32m    508\u001b[0m ArrayList \u001b[38;5;241m=\u001b[39m JavaClass(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjava.util.ArrayList\u001b[39m\u001b[38;5;124m\"\u001b[39m, gateway_client)\n\u001b[1;32m    509\u001b[0m java_list \u001b[38;5;241m=\u001b[39m ArrayList()\n\u001b[0;32m--> 510\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melement\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjava_list\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m java_list\n",
      "File \u001b[0;32m~/Desktop/development/data-engineering/BigdataEnv/lib/python3.12/site-packages/pyspark/sql/column.py:718\u001b[0m, in \u001b[0;36mColumn.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 718\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    719\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_ITERABLE\u001b[39m\u001b[38;5;124m\"\u001b[39m, message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjectName\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m    720\u001b[0m     )\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m: [NOT_ITERABLE] Column is not iterable."
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import instr,length,substring,lit,expr\n",
    "df2 = df.withColumn(\"startInd\",instr(df.email,\"@\")).withColumn(\"endInd\",length(df.email))\n",
    "df2.show()\n",
    "\n",
    "df3 = df.withColumn(\"answer\",expr(\"substring(email,instr(email,'@'),length(email))\"))\n",
    "df3.show()\n",
    "\n",
    "df4 = df2.withColumn(\"answer2\",expr(\"substring(email,startInd,endInd)\"))\n",
    "df4.show()\n",
    "\n",
    "df5 = df2.withColumn(\"answer3\",substring(df2.email,col(\"startInd\").cast('integer'),col(\"endInd\").cast('integer')))\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- email: string (nullable = true)\n",
      " |-- startInd: integer (nullable = true)\n",
      " |-- endInd: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m substr,length\n\u001b[0;32m----> 3\u001b[0m df\u001b[38;5;241m.\u001b[39mselect(df\u001b[38;5;241m.\u001b[39memail\u001b[38;5;241m.\u001b[39msubstr(Ind,\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memail\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memail\u001b[49m\u001b[43m)\u001b[49m))\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import substr,length\n",
    "\n",
    "df.select(df.email.substr(Ind,df.email.len(df.email))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[NOT_SAME_TYPE] Argument `startPos` and `length` should be the same type, got Column and int.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m instr\n\u001b[0;32m----> 3\u001b[0m df2 \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdomain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memail\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubstr\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memail\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m@\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Desktop/development/data-engineering/BigdataEnv/lib/python3.12/site-packages/pyspark/sql/column.py:919\u001b[0m, in \u001b[0;36mColumn.substr\u001b[0;34m(self, startPos, length)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;124;03mReturn a :class:`Column` which is a substring of the column.\u001b[39;00m\n\u001b[1;32m    893\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;124;03m[Row(col='Ali'), Row(col='Bob')]\u001b[39;00m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(startPos) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mtype\u001b[39m(length):\n\u001b[0;32m--> 919\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    920\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_SAME_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    921\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    922\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name1\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstartPos\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    923\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name2\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    924\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type1\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(startPos)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m    925\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type2\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(length)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m    926\u001b[0m         },\n\u001b[1;32m    927\u001b[0m     )\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(startPos, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    929\u001b[0m     jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jc\u001b[38;5;241m.\u001b[39msubstr(startPos, length)\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m: [NOT_SAME_TYPE] Argument `startPos` and `length` should be the same type, got Column and int."
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import instr\n",
    "\n",
    "df2 = df.withColumn(\"domain\",df.email.substr(instr(df.email,\"@\"),11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+\n",
      "|          email|domain|\n",
      "+---------------+------+\n",
      "|padhu@gmail.com|  NULL|\n",
      "+---------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BigdataEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
