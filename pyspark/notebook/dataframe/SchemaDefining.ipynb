{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom schema defining using the StructType and StructField"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need for StructType and StructField :\n",
    "1. For creating complex data structure\n",
    "2. Enforcing the input data structure\n",
    "3. For creating empty dataframe without schema\n",
    "4. overide default config while importing data from file \n",
    "Eg: data type are infered from the imported data and nullable field are default to True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/03 11:01:11 WARN Utils: Your hostname, padmanabhan-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "24/11/03 11:01:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/03 11:01:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"learn\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# creating empty df using the StructType\n",
    "from pyspark.sql.types import StructType,StructField\n",
    "emptyDf = spark.createDataFrame([],StructType())\n",
    "print(emptyDf.isEmpty())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = false)\n",
      " |-- amount: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# defining data structure for the data\n",
    "\n",
    "from pyspark.sql.types import StringType,IntegerType\n",
    "\n",
    "simpleData = [(\"padhu\",1000),(\"karthir\",5000)]\n",
    "\n",
    "schema = StructType([\n",
    "            StructField(\"name\",StringType(),False),\n",
    "            StructField('amount',IntegerType(),True)\n",
    "])\n",
    "\n",
    "simpleDf = spark.createDataFrame(simpleData,schema=schema)\n",
    "simpleDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = false)\n",
      " |    |-- firstName: string (nullable = true)\n",
      " |    |-- middleName: string (nullable = true)\n",
      " |    |-- lastName: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Schema for complex nested data structure\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType\n",
    "complexData = [\n",
    "    ((\"James\",\"\",\"Smith\"),\"36636\",\"M\",3100),\n",
    "    ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",4300),\n",
    "    ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",1400),\n",
    "    ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",5500),\n",
    "    ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",-1)\n",
    "  ]\n",
    "\n",
    "complexSchema = StructType([\n",
    "                StructField(\"name\",StructType([\n",
    "                    StructField(\"firstName\",StringType(),True),\n",
    "                    StructField(\"middleName\",StringType(),True),\n",
    "                    StructField(\"lastName\",StringType(),True)\n",
    "                ]),False),\n",
    "                StructField(\"id\",StringType(),True),\n",
    "                StructField(\"gender\",StringType(),True),\n",
    "                StructField(\"salary\",IntegerType(),True),\n",
    "])\n",
    "\n",
    "complexDF = spark.createDataFrame(complexData,complexSchema)\n",
    "complexDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = false)\n",
      " |    |-- firstName: string (nullable = true)\n",
      " |    |-- middleName: string (nullable = true)\n",
      " |    |-- lastName: string (nullable = true)\n",
      " |-- OtherInfo: struct (nullable = false)\n",
      " |    |-- new id: string (nullable = true)\n",
      " |    |-- new gender: string (nullable = true)\n",
      " |    |-- new salary: integer (nullable = true)\n",
      " |    |-- salary grade: string (nullable = false)\n",
      "\n",
      "+--------------------+------------------------+\n",
      "|name                |OtherInfo               |\n",
      "+--------------------+------------------------+\n",
      "|{James, , Smith}    |{36636, M, 3100, medium}|\n",
      "|{Michael, Rose, }   |{40288, M, 4300, high)} |\n",
      "|{Robert, , Williams}|{42114, M, 1400, low}   |\n",
      "|{Maria, Anne, Jones}|{39192, F, 5500, high)} |\n",
      "|{Jen, Mary, Brown}  |{, F, -1, low}          |\n",
      "+--------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adding and changing struct of the DF\n",
    "from pyspark.sql.functions import col,struct,when\n",
    "\n",
    "updatedComplexDF = complexDF.withColumn(\"OtherInfo\",\n",
    "                                        struct(col(\"id\").alias(\"new id\"),\n",
    "                                               col(\"gender\").alias(\"new gender\"),\n",
    "                                               col(\"salary\").alias(\"new salary\"),\n",
    "                                               when(col(\"salary\").cast(IntegerType())<2000,\"low\")\n",
    "                                               .when(col(\"salary\").cast(IntegerType())<4000,\"medium\")\n",
    "                                               .otherwise(\"high)\").alias(\"salary grade\")\n",
    "                                               )\n",
    "                                        ).drop(\"id\",\"salary\",\"gender\")\n",
    "\n",
    "updatedComplexDF.printSchema()\n",
    "updatedComplexDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = false)\n",
      " |-- languages: array (nullable = false)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- identification: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = false)\n",
      "\n",
      "+-----------+--------------+-----------------------------------+\n",
      "|name       |languages     |identification                     |\n",
      "+-----------+--------------+-----------------------------------+\n",
      "|padmanabhan|[python, java]|{hair -> black, company -> walmart}|\n",
      "|thilak     |[c++, java]   |{hair -> brown, company -> walmart}|\n",
      "+-----------+--------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ArrayType and Map Type \n",
    "from pyspark.sql.types import ArrayType,MapType,StringType,StructType,StructField\n",
    "\n",
    "sampleData = [\n",
    "    (\"padmanabhan\",[\"python\",\"java\"],{\"hair\":\"black\",\"company\":\"walmart\"}),\n",
    "    (\"thilak\",[\"c++\",\"java\"],{\"hair\":\"brown\",\"company\":\"walmart\"})\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "            StructField(\"name\",StringType(),False),\n",
    "            StructField(\"languages\",ArrayType(StringType()),False),\n",
    "            StructField(\"identification\",MapType(StringType(),StringType(),False))\n",
    "])\n",
    "\n",
    "sampleDF = spark.createDataFrame(sampleData,schema)\n",
    "sampleDF.printSchema()\n",
    "sampleDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Schema to Json\n",
    "\n",
    "schema_data = sampleDF.schema.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/padmanabhan/Desktop/development/data-engineering/pyspark/samples/schema_data_output.json\",\"w+\") as file:\n",
    "    file.write(schema_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"fields\":[{\"metadata\":{},\"name\":\"name\",\"nullable\":false,\"type\":\"string\"},{\"metadata\":{},\"name\":\"languages\",\"nullable\":false,\"type\":{\"containsNull\":true,\"elementType\":\"string\",\"type\":\"array\"}},{\"metadata\":{},\"name\":\"identification\",\"nullable\":true,\"type\":{\"keyType\":\"string\",\"type\":\"map\",\"valueContainsNull\":false,\"valueType\":\"string\"}}],\"type\":\"struct\"}\n"
     ]
    }
   ],
   "source": [
    "print(schema_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'struct<name:string,languages:array<string>,identification:map<string,string>>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternative way to get DF schema as simple string\n",
    "\n",
    "sampleDF.schema.simpleString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = false)\n",
      " |-- languages: array (nullable = false)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- identification: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+-----------------------------------+\n",
      "|name       |languages     |identification                     |\n",
      "+-----------+--------------+-----------------------------------+\n",
      "|padmanabhan|[python, java]|{hair -> black, company -> walmart}|\n",
      "|thilak     |[c++, java]   |{hair -> brown, company -> walmart}|\n",
      "+-----------+--------------+-----------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Creating DF using the schema stored in the Json file\n",
    "import json\n",
    "\n",
    "with open(\"/home/padmanabhan/Desktop/development/data-engineering/pyspark/samples/schema_data_output.json\", \"r\") as f:\n",
    "    schema_json = json.load(f)\n",
    "\n",
    "schemaFromJson  = StructType.fromJson(schema_json)\n",
    "\n",
    "DFFromJson = spark.createDataFrame(sampleData,schema=schemaFromJson)\n",
    "\n",
    "DFFromJson.printSchema()\n",
    "DFFromJson.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'StructType' has no attribute 'fromDDL'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Creating StrucType using the DDL\u001b[39;00m\n\u001b[1;32m      3\u001b[0m ddlSchemaStr \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`fullName` STRUCT<`first`: STRING, `last`: STRING,`middle`: STRING>,`age` INT,`gender` STRING\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m ddlSchema \u001b[38;5;241m=\u001b[39m \u001b[43mStructType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromDDL\u001b[49m(ddlSchemaStr)\n\u001b[1;32m      5\u001b[0m ddlSchema\u001b[38;5;241m.\u001b[39mprintTreeString()\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'StructType' has no attribute 'fromDDL'"
     ]
    }
   ],
   "source": [
    "# Creating StrucType using the DDL\n",
    "\n",
    "ddlSchemaStr = \"`fullName` STRUCT<`first`: STRING, `last`: STRING,`middle`: STRING>,`age` INT,`gender` STRING\"\n",
    "ddlSchema = StructType.fromDDL(ddlSchemaStr)\n",
    "ddlSchema.printTreeString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exist\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# check for existence of a column in the DF\n",
    "\n",
    "if \"name\" in sampleDF.columns:\n",
    "    print(\"exist\")\n",
    "else:\n",
    "    print(\"do not exist\")\n",
    "\n",
    "# alternatives to check for the existence of the column\n",
    "\n",
    "print(\"name\" in sampleDF.schema.fieldNames())\n",
    "print(StructField(\"name\",StringType(),False) in sampleDF.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BigdataEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
